\documentclass{beamer}
\usetheme{Boadilla}

\usepackage{amsmath}
\usepackage{array}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{biblatex}
\graphicspath{ {./images/} }


\title{Graph Auto-regressive model}
\author{Ksenofontov Gregory}
\institute{MIPT}


\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Auto-regressive (AR) model}
    Let it is given past $p$ time steps $x^p_t=[x_t, x_{t-1}, \dots, x_{t-p+1}]$
    So, the next observation is generated by this process:
    \begin{equation}
        x_{t+1} = f(x^p_t) + \epsilon, \epsilon \sim \mathcal{N}(0, \sigma^2)
    \label{eq:ar}
    \end{equation}
    where $f(x^p_t)$ - AR function that could be parameterised somehow. \par
    So, the prediction is given by $\hat x_{t+1} = \mathbb{E}_\epsilon\left[x_{t+1}\right] = f(x^p_t)$ \par
    However we want to predict next graph $g_{t+1}$, so lets generalize this to graph setting\footnote{\href{https://arxiv.org/pdf/1903.07299.pdf}{Autoregressive Models for Sequences of Graphs}}.
\end{frame}
\begin{frame}{Graph Auto-regressive model}
    Firstly, lets generalize generation process.\par
    We have graph space
    $\mathcal{G} = \left(\mathcal{V}, \mathcal{E}\right)$
    where 
    $$\mathcal{V} = \{v_i\in \mathbb{R}^F\}_{i=1}^N, \mathcal{E} = \{e_i\in \mathbb{R}^S\}_{v_i, v_j \in \mathcal{V}}$$
    The past $p$ time steps are $g^p_t=[g_t, g_{t-1}, \dots, g_{t-p+1}]$, where $\forall g\in \mathcal{G}$
    So, the next observation is given by:
    \begin{equation}
        g_{t+1} = H\left(\phi(g^p_t), \eta\right), \eta\sim Q(g), H:\mathcal{G}\xrightarrow{}\mathcal{G}
    \end{equation}
    where $H$ is function that effects noise on graph, $\eta$ is noise graph and $Q(g)$ is a graph distribution defined on the Borel sets of space $(\mathcal{G}, \mathrm{d})$\par
    So, the prediction is given by $$\hat g_{t+1} = \arg\min_{g'\in\mathcal{G}}\int_\mathcal{G}\mathrm{d}( g', g_{t+1})^2dQ(g)= \mathbb{E}^f_\eta\left[g_{t+1}\right] =  \phi(g^p_t),$$ 
    where $\mathbb{E}^f_\eta\left[\cdot\right]$ is Frechet mean, $\mathrm{d}(\cdot, \cdot)$ is pre-metric
    
\end{frame}
\begin{frame}{Learning the AR function with a GNN}
    Authors\footnote{\href{https://arxiv.org/pdf/1903.07299.pdf}{Autoregressive Models for Sequences of Graphs}} propose one of the possible architecture (Figure \ref{fig:arch}) that can parameterize AR function.
    \begin{figure}
        \centering
        \includegraphics[scale=0.2]{images/architecture.png}
        \caption{Proposed architecture}
        \label{fig:arch}
     \end{figure}
    In this case by mapping graphs to a vector space, we go back to the numerical setting of (\ref{eq:ar}) 
\end{frame}
\end{document}